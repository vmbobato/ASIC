{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import zlib\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervisely data uses JSON file with Bitmaps for each class title for their segmentations.\n",
    "This script reads the bitmap and converts it to masks based on the colors for the classes.\n",
    "This outputs a *_color.png used for visualization of the masks,\n",
    "              *_masks.png containing a class for each pixel.\n",
    "              for each subdirectory (train, valid, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE:\n",
    "\n",
    "\"classTitle\": \"urban_land\",\n",
    "            \"bitmap\": {\n",
    "                \"data\": \"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\","
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping for Colors and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\n",
    "    \"urban_land\": 0,\n",
    "    \"agriculture_land\": 1,\n",
    "    \"rangeland\": 2,\n",
    "    \"forest_land\": 3,\n",
    "    \"water\": 4,\n",
    "    \"barren_land\": 5,\n",
    "    \"unknown\": 6\n",
    "}\n",
    "color_map = np.array([\n",
    "    [0, 255, 255],     # urban_land - Cyan\n",
    "    [255, 255, 0],     # agriculture_land - Yellow\n",
    "    [255, 0, 255],     # rangeland - Magenta\n",
    "    [0, 255, 0],       # forest_land - Green\n",
    "    [0, 0, 255],       # water - Blue\n",
    "    [255, 255, 255],   # barren_land - White\n",
    "    [128, 128, 128]    # unknown - Gray\n",
    "], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_bitmap(data_str):\n",
    "    raw = base64.b64decode(data_str)\n",
    "    decompressed = zlib.decompress(raw)\n",
    "    mask_img = Image.open(BytesIO(decompressed)).convert(\"L\")\n",
    "    return np.array(mask_img)\n",
    "\n",
    "def convert_supervisely_to_mask(img_shape, objects):\n",
    "    h, w = img_shape\n",
    "    full_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    for obj in objects:\n",
    "        class_name = obj[\"classTitle\"]\n",
    "        if class_name not in class_map:\n",
    "            continue\n",
    "        class_id = class_map[class_name]\n",
    "        bitmap = obj[\"bitmap\"]\n",
    "        origin_x, origin_y = bitmap[\"origin\"]\n",
    "        mask = decode_bitmap(bitmap[\"data\"])\n",
    "        binary_mask = (mask > 127)\n",
    "        class_mask = np.zeros_like(mask, dtype=np.uint8)\n",
    "        class_mask[binary_mask] = class_id\n",
    "        region = full_mask[origin_y:origin_y+mask.shape[0], origin_x:origin_x+mask.shape[1]]\n",
    "        region[binary_mask] = class_mask[binary_mask]\n",
    "        full_mask[origin_y:origin_y+mask.shape[0], origin_x:origin_x+mask.shape[1]] = region\n",
    "    return full_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LICENSE.md', 'meta.json', 'README.md', 'test', 'train', 'valid']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('deepglobe-land-cover-2018-DatasetNinja')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the train dataset from DeepGlobe 2018 Dataset\n",
    "    - Saves all output temporarily in DeepGlobe_Converted_Dataset/full/\n",
    "    - Only the train folder in the original dataset has bitmaps, so only train/ is used for conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete.\n"
     ]
    }
   ],
   "source": [
    "input_base = \"deepglobe-land-cover-2018-DatasetNinja\"\n",
    "input_img_dir = os.path.join(input_base, \"train\", \"img\")\n",
    "input_ann_dir = os.path.join(input_base, \"train\", \"ann\")\n",
    "output_base = \"DeepGlobe_Converted_Dataset\"\n",
    "temp_dir = os.path.join(output_base, \"full\")\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(input_ann_dir):\n",
    "    if file.endswith(\".json\"):\n",
    "        name = file.replace(\"_sat.jpg.json\", \"\")\n",
    "        img_path = os.path.join(input_img_dir, f\"{name}_sat.jpg\")\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Image file not found for annotation {file}\")\n",
    "            continue\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        width, height = img.size\n",
    "        ann_path = os.path.join(input_ann_dir, file)\n",
    "\n",
    "        with open(ann_path, \"r\") as f:\n",
    "            ann = json.load(f)\n",
    "\n",
    "        mask = convert_supervisely_to_mask((height, width), ann[\"objects\"])\n",
    "\n",
    "        img.save(os.path.join(temp_dir, f\"{name}_sat.jpg\"))\n",
    "        Image.fromarray(mask).save(os.path.join(temp_dir, f\"{name}_mask.png\"))\n",
    "        color_mask = color_map[mask]\n",
    "        Image.fromarray(color_mask).save(os.path.join(temp_dir, f\"{name}_color.png\"))\n",
    "\n",
    "print(\"Conversion complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data 85/15 for training and validation\n",
    "    - DeepGlobe_Converted_Dataset/train/\n",
    "    - DeepGlobe_Converted_Dataset/valid/\n",
    "    - Leaves /full/ empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 682 samples in train/, 121 in valid/.\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(output_base, \"train\")\n",
    "valid_dir = os.path.join(output_base, \"valid\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(valid_dir, exist_ok=True)\n",
    "\n",
    "triplets = sorted([f.replace(\"_sat.jpg\", \"\") for f in os.listdir(temp_dir) if f.endswith(\"_sat.jpg\")])\n",
    "random.shuffle(triplets)\n",
    "split_idx = int(0.85 * len(triplets))\n",
    "train_ids = triplets[:split_idx]\n",
    "valid_ids = triplets[split_idx:]\n",
    "\n",
    "def move_triplet(name, dest):\n",
    "    for suffix in [\"_sat.jpg\", \"_mask.png\", \"_color.png\"]:\n",
    "        src = os.path.join(temp_dir, f\"{name}{suffix}\")\n",
    "        dst = os.path.join(dest, f\"{name}{suffix}\")\n",
    "        if os.path.exists(src):\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "for name in train_ids:\n",
    "    move_triplet(name, train_dir)\n",
    "\n",
    "for name in valid_ids:\n",
    "    move_triplet(name, valid_dir)\n",
    "\n",
    "print(f\"Done. {len(train_ids)} samples in train/, {len(valid_ids)} in valid/.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in mask: [0 1 2 4]\n",
      "[[2 2 2 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "mask = np.array(Image.open(\"DeepGlobe_Converted_Dataset/train/119_mask.png\"))\n",
    "print(\"Unique values in mask:\", np.unique(mask))\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "892\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir('data/train')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
